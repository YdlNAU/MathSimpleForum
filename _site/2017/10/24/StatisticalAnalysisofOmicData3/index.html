<html>
<head>
<META http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta content="text/html; charset=utf-8" http-equiv="Content-Type">
<meta content="text/css" http-equiv="Content-Style-Type">
<title>multivariate
 supervised analysis</title>
</head>
<body>
<h1 align="center" class="root">
<a name="3ng1c2ci4fsqc7214f4stmgmpd">multivariate
 supervised analysis</a>
</h1>
<div align="center" class="globalOverview">
<img src="Andrew-3_files/images/multivariate  supervised analysis 2.jpg"></div>
<h2 class="topic">
<a name="0be3riak89qu3l0okemnrrh1rn">classification</a>
</h2>
<h3 class="topic">
<a name="56rq7k42b5vaonfmj7ecv687jg">&nbsp;概念</a>
</h3>
<h3 class="topic">
<a name="6v7146no9er73ksm9evpenevc7">&nbsp;&nbsp;已经分类好，把sample放进去
given a sample, to predict its class given a set of a-priori classes</a>
</h3>
<h3 class="topic">
<a name="4gqneau00dappa7s4ehh1hq1v5">&nbsp;&nbsp;&nbsp;与clustering完全是两码事啦</a>
</h3>
<h3 class="topic">
<a name="0dg45n4iaq0iu7g1ivdc5ppmng">&nbsp;&nbsp;decision boundaries 可以是线性也可是非线性的。 decision boundaries需要训练!!!</a>
</h3>
<h3 class="topic">
<a name="51cqhsnn8rn73g3flg92o2pldd">&nbsp;参数</a>
</h3>
<h3 class="topic">
<a name="0a4bk28hbd1ikseao32lnsh58r">&nbsp;&nbsp;input vector: x.</a>
</h3>
<h3 class="topic">
<a name="1qcjh3d4e4vpvofc3qa2ftkphh">&nbsp;&nbsp;放进k个class C1,C2,...,Ck</a>
</h3>
<h3 class="topic">
<a name="14vnb0fra4031iel7bs4dpo1rn">&nbsp;&nbsp;learning/ training set (LS)</a>
</h3>
<h3 class="topic">
<a name="338tstb6v5fjobaqpv09c3to5e">&nbsp;&nbsp;&nbsp;L = (x1, Y1), ..., (xn, Yn)</a>
</h3>
<h3 class="topic">
<a name="5aj2fghlpjg6nf4tngfdmf71v9">&nbsp;Bayes theorem</a>
</h3>
<h3 class="topic">
<a name="2q5rmhhom9hmuv5mcr6lsgn0i7">&nbsp;&nbsp;估计p (Ck | x)</a>
</h3>
<h3 class="topic">
<a name="4gc1o4sail5d78o37dprfrc72q">&nbsp;&nbsp;&nbsp;input是x的sample属于class k的概率</a>
</h3>
<h3 class="topic">
<a name="4eur1lanjk3p5ooqged8vjd1fi">&nbsp;&nbsp;公式 slide7</a>
</h3>
<h3 class="topic">
<a name="2r3q6r0e4r0bol7rf0i6nshp92">&nbsp;decision theory</a>
</h3>
<h3 class="topic">
<a name="5p59c98msaa2cscrg9assr1pj6">&nbsp;&nbsp;原则：选择使p(Ck|x)最大的k</a>
</h3>
<h3 class="topic">
<a name="5ab3l65boldrptkkapa33rl0ut">&nbsp;&nbsp;p(x,C1)=p(x,C2)，error最小。（在K=2， input一维时，slide 8的graph很好理解）</a>
</h3>
<h3 class="topic">
<a name="73ajd1nik8vnd7mlmoih3v2cnr">&nbsp;model and parameter selection</a>
</h3>
<h3 class="topic">
<a name="69100clccjf34mfc76hpojsaud">&nbsp;&nbsp;分割为单位格，要求数据量足够大。维度越高，需要的数据越多</a>
</h3>
<h3 class="topic">
<a name="2a072vbbnbhdc9dcvvjp0rf3tk">&nbsp;&nbsp;&nbsp;curse of dimensionality</a>
</h3>
<h3 class="topic">
<a name="3lr7h27a8d1l0cnaobl3m2294a">&nbsp;&nbsp;n个data，D=n-1个维度，总能找到D-1种分割成两类的方法</a>
</h3>
<h3 class="topic">
<a name="7tqfdp1co2pl9nf3k5p8rab9vg">&nbsp;&nbsp;Model参数越多(# parameters)，复杂度(complexity)越高</a>
</h3>
<h3 class="topic">
<a name="78jcmh218d51tgm26h2pndc4cp">&nbsp;&nbsp;&nbsp;Ex: polynomial curve fitting</a>
</h3>
<h3 class="topic">
<a name="4gcmpmr85q250nt0hdovv04ue0">&nbsp;&nbsp;&nbsp;&nbsp;衡量over-fitting： RMS(Root-Mean-Square) Error     slide17</a>
</h3>
<h3 class="topic">
<a name="6q5fftc2msp08d7hfrjbmjpop7">&nbsp;&nbsp;&nbsp;&nbsp;Complexity parameter   slide20</a>
</h3>
<h3 class="topic">
<a name="29b5k60mlse419v6u3np9t1t92">&nbsp;&nbsp;&nbsp;&nbsp;penalty-term   slide 20</a>
</h3>
<h3 class="topic">
<a name="6h7n03j0skbqlgj0grku80hnk2">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;λ控制了模型的复杂度</a>
</h3>
<h3 class="topic">
<a name="4i5susr1opjqua30gthndgfgob">&nbsp;cross-validation</a>
</h3>
<h3 class="topic">
<a name="2ge9ul39oo9lo9oooud3iqh0op">&nbsp;&nbsp;目的：在样本量不太大的情况下，避免overfitting</a>
</h3>
<h3 class="topic">
<a name="7f9ughbfmrfgsp8lmhr6p1o0u5">&nbsp;&nbsp;所有的样本都会被用于training和testing</a>
</h3>
<h3 class="topic">
<a name="19aropnajv66q4drmon8ssqdrj">&nbsp;&nbsp;&nbsp;slide22 </a>
</h3>
<h2 class="topic">
<a name="30858gmo84t6farat88dhvo8ee">shrunken centroids classifier</a>
</h2>
<h3 class="topic">
<a name="43a87jen7pnbneh2tj4qu0ak1o">&nbsp;naive centroid</a>
</h3>
<h3 class="topic">
<a name="3kkfjheahrp8b2ph55vj2dgih0">&nbsp;&nbsp;centroid = 一个class中所有sample的均值 </a>
</h3>
<h3 class="topic">
<a name="6fch0r57b221i8sqv74d0qv50e">&nbsp;&nbsp;选择使|y-Ck|^2 最小的k</a>
</h3>
<h3 class="topic">
<a name="5jrmcnlo37rbqqipmbr82nb0jb">&nbsp;&nbsp;存在问题: 容易overfitting，因为很多noise</a>
</h3>
<h3 class="topic">
<a name="0hfnhcjqig3qubjhsa3bjn47td">&nbsp;&nbsp;解决方法： shrink 即只有最相关的基因才会被纳入模型，如slide25中的红色基因</a>
</h3>
<h3 class="topic">
<a name="67aafs6s7vho434qn71l8svnf8">&nbsp;公式 slide26</a>
</h3>
<h3 class="topic">
<a name="5pm10gl9u60qck7af8dvonnt3v">&nbsp;&nbsp;定义△&gt;=dgk时。d'gk=0，则c&lsquo;gk=&lt;xg&gt;， 这样的基因不重要</a>
</h3>
<h3 class="topic">
<a name="4isl1qvljecpa541bsgs0793og">&nbsp;&nbsp;&nbsp;△：shrinkage parameter</a>
</h3>
<h3 class="topic">
<a name="4r5m6pbfjgv9kerpgqp3ih9937">&nbsp;&nbsp;基因越重要，dgk越大</a>
</h3>
<h3 class="topic">
<a name="3ajqbli298o5r9513bojdguou1">&nbsp;&nbsp;可以把dgk理解为regularized t statistic</a>
</h3>
<h3 class="topic">
<a name="1e5dvp71nif4g38sj15vrflb3s">&nbsp;&nbsp;选择△的方法</a>
</h3>
<h3 class="topic">
<a name="10qd3drgk6oqvdgrihh7cpuc3o">&nbsp;&nbsp;&nbsp;使CV error最小，同时基因数最少（最简单）</a>
</h3>
<h3 class="topic">
<a name="537g0aa6labma7b038q2cig5l1">&nbsp;class membership</a>
</h3>
<h3 class="topic">
<a name="100o1ngim9jsa7gm2aruum66qn">&nbsp;&nbsp;公式 slide30</a>
</h3>
<h3 class="topic">
<a name="29um3pb93u6ejvg578bfdino5j">&nbsp;&nbsp;把样本分到某一class的概率</a>
</h3>
<h2 class="topic">
<a name="77hh21ll047mihasdnqi23ug9m">semi-supervised PCA/SVD</a>
</h2>
<h3 class="topic">
<a name="53lujgt4e3ilji4q2bq2fkljbh">&nbsp;保证SVD中的基因 与我们感兴趣的基因相关</a>
</h3>
<h3 class="topic">
<a name="4nsm97pkmjmc9d3s9i1ujbs6m5">&nbsp;训练集X，先选中POI相关的基因，再对这些选出的基因（m个）做SVD</a>
</h3>
<h3 class="topic">
<a name="05mhp7bhtfkccbhqr2ud6s6iif">&nbsp;&nbsp;实际上，就是只对挑选出的top-PC做SVD</a>
</h3>
<h3 class="topic">
<a name="42t6b0dtk9492g3sopgg6sc39k">&nbsp;&nbsp;m = complexity parameter ,可用CV优化此参数</a>
</h3>
<h3 class="topic">
<a name="63s9ik4v1e6gt46bho2db4d3a8">&nbsp;&nbsp;流程示意图 slide35-36</a>
</h3>
</body>
</html>
